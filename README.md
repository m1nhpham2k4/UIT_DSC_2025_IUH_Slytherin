# ğŸ UIT_DSC_2025_IUH_Slytherin

---

## âš ï¸ Note
Due to the use of **Large Language Models (LLMs)**, the output might slightly vary across runs (some predictions may differ by a few samples).

---

## ğŸ§  Overview

This project is a **Vietnamese Hallucination Detection System**, designed to classify whether a model-generated response is consistent, distorted, or hallucinated relative to its context.

The system is built upon the **Qwen3-4B-Base** model and fine-tuned using **LoRA (Low-Rank Adaptation)** to optimize memory usage and training speed.

A custom **`lm_head` modification strategy** is applied â€” only retaining weight parameters corresponding to label tokens (`1`, `2`, `3`) during training.  
This helps the model focus on **classification** rather than **text generation**.  
After fine-tuning, the full `lm_head` is restored for precise **inference**.

During inference, **dynamic bias calibration** is used to balance prediction probabilities among three label classes:

- **`class 1 â€“ no`**: Response is fully consistent with the context.  
- **`class 2 â€“ intrinsic`**: Response contradicts or distorts the context.  
- **`class 3 â€“ extrinsic`**: Response adds external information not present in the context.

---

## Kiáº¿n trÃºc thÆ° má»¥c
```
UIT_DSC_2025_IUH_SLYTHERIN/
â”‚
â”œâ”€â”€ data/                         # ğŸ“‚ Dá»¯ liá»‡u gá»‘c dÃ¹ng cho huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­
â”‚   â”œâ”€â”€ test/                     # ğŸ“ Táº­p dá»¯ liá»‡u kiá»ƒm thá»­
â”‚   â”‚   â”œâ”€â”€ vihallu-private-test.csv
â”‚   â”‚   â””â”€â”€ vihallu-public-test.csv
â”‚   â””â”€â”€ train/                    # ğŸ“ Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n
â”‚       â””â”€â”€ vihallu-train.csv
â”‚
â”œâ”€â”€ src/                          # ğŸ“‚ MÃ£ nguá»“n chÃ­nh cá»§a dá»± Ã¡n
â”‚   â”œâ”€â”€ data_utils/               # ğŸ§© Tiá»‡n Ã­ch xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ collator.py           # Äá»‹nh nghÄ©a hÃ m collate_fn cho DataLoader
â”‚   â”‚   â”œâ”€â”€ dataset_builder.py    # XÃ¢y dá»±ng Dataset tá»« file CSV
â”‚   â”‚   â””â”€â”€ preprocess.py         # HÃ m tiá»n xá»­ lÃ½ dá»¯ liá»‡u (chuáº©n hÃ³a, lÃ m sáº¡ch, tokenize,â€¦)
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                   # ğŸ§  CÃ¡c mÃ´ hÃ¬nh hoáº·c module liÃªn quan Ä‘áº¿n LLM
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lm_head_utils.py      # Tiá»‡n Ã­ch cho pháº§n Linear head cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯
â”‚   â”‚   â””â”€â”€ qwen3_instruct.py     # Cáº¥u hÃ¬nh hoáº·c triá»ƒn khai mÃ´ hÃ¬nh Qwen3-Instruct
â”‚   â”‚
â”‚   â”œâ”€â”€ prompt/                   # ğŸ’¬ Xá»­ lÃ½ prompt cho huáº¥n luyá»‡n/inference
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_prompt.py        # Cáº¥u trÃºc prompt cÆ¡ báº£n
â”‚   â”‚   â””â”€â”€ format_prompt.py      # Äá»‹nh dáº¡ng vÃ  xÃ¢y dá»±ng prompt cho tá»«ng nhiá»‡m vá»¥
â”‚   â”‚
â”‚   â””â”€â”€ tasks/                    # ğŸš€ CÃ¡c tÃ¡c vá»¥ chÃ­nh cá»§a project
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ inference.py          # Cháº¡y suy luáº­n (inference)
â”‚       â””â”€â”€ train.py              # Script huáº¥n luyá»‡n mÃ´ hÃ¬nh
â”‚
â””â”€â”€ LICENSE
â””â”€â”€ README.mb
```

## âš™ï¸ Installation

### 1. System Requirements

- **Python** â‰¥ 3.10  
- **CUDA-compatible GPU** (â‰¥ 32GB VRAM recommended)  
- Example hardware: NVIDIA RTX 5090 (32GB VRAM), 32GB RAM, 512GB SSD

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

```csv
id,context,prompt,response,label
1,"<context text>","<question>","<response>","no"
2,"<context text>","<question>","<response>","intrinsic"
2,"<context text>","<question>","<response>","extrinsic"
```
## ğŸ“Š Data Format

### Training Data (CSV)

**Required Columns:**
- `id`: Unique sample ID
- `context`: Background or reference text
- `prompt`: Instruction or question
- `response`: Response from csv
- `label`: Classification label(`no`, `intrinsic`, `extrinsic`)

## ğŸ›ï¸ Configuration Parameters

### Training Parameters
| Parameter | Default | Description |
|-----------|----------|-------------|
| `--model_name` | `unsloth/Qwen3-4B-Base` | Base model |
| `--max_seq_len` | 3072 | Maximum sequence length |
| `--epochs` | 1 | Number of training epochs |
| `--per_device_train_batch_size` | 4 | Batch size per GPU |
| `--gradient_accumulation_steps` | 8 | Gradient accumulation steps |
| `--lr` | 1e-4 | Learning rate |
| `--save_steps` | 500 | Number of steps between checkpoint saves |

### LoRA Parameters

| Parameter | Default | Description |
|-----------|----------|-------------|
| `--lora_r` | 16 | LoRA rank |
| `--lora_alpha` | 16 | LoRA alpha scaling factor |
| `--lora_dropout` | 0 | Dropout rate |
